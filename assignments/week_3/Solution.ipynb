{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.genfromtxt('../week_2/MNIST/mnist_train.csv',delimiter=',')\n",
    "test_data=np.genfromtxt('../week_2/MNIST/mnist_test.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=train_data[:,0]\n",
    "train_set=train_data[:,1:]\n",
    "test_labels=test_data[:,0]\n",
    "test_set=test_data[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_set))\n",
    "print(np.shape(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc=preprocessing.OneHotEncoder(sparse=False,categories='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories='auto',\n",
       "       dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "       n_values=None, sparse=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(train_labels.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_onehot=enc.transform(train_labels.reshape(-1,1))\n",
    "test_labels_onehot=enc.transform(test_labels.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.reshape(60000,28,28,1)\n",
    "test_set = test_set.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN:\n",
    "    def __init__(self, wd_factor, learning_rate):\n",
    "        self.wd_factor = wd_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_pointer = 0\n",
    "        self.test_pointer = 0\n",
    "        \n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28, 1], name='input')\n",
    "        self.ground_truth = tf.placeholder(dtype=tf.float32, shape=[None, 10], name='ground_truth')\n",
    "        \n",
    "        # For batch norm and dropout\n",
    "        self.is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        print(self.input)\n",
    "        \n",
    "        self._build_graph()\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        weights = []  # for weight decay\n",
    "        \n",
    "        with tf.variable_scope('layers'):\n",
    "            h = tf.layers.conv2d(self.input, 28, (11, 11), strides=(4, 4), padding='same', \n",
    "                                 data_format='channels_last', activation=None, use_bias=True,\n",
    "                                 kernel_initializer=tf.glorot_uniform_initializer(), name='conv1')\n",
    "            print(h)\n",
    "            \n",
    "            h = tf.layers.batch_normalization(h, training=self.is_training)\n",
    "            h = tf.nn.relu(h)\n",
    "            h = tf.layers.conv2d(h, 56, (5, 5), strides=(1, 1), padding='same', \n",
    "                                 data_format='channels_last', activation=None, use_bias=True,\n",
    "                                 kernel_initializer=tf.glorot_uniform_initializer(), name='conv2')\n",
    "            \n",
    "            h = tf.layers.batch_normalization(h, training=self.is_training)\n",
    "            h = tf.nn.relu(h)\n",
    "            h = tf.layers.conv2d(h, 56, (3, 3), strides=(1, 1), padding='same', \n",
    "                                 data_format='channels_last', activation=None, use_bias=True,\n",
    "                                 kernel_initializer=tf.glorot_uniform_initializer(), name='conv3')\n",
    "            \n",
    "            # Downsample\n",
    "            h = tf.layers.max_pooling2d(h, (2, 2), (2, 2), padding='valid', name='pool1')\n",
    "            print(h)\n",
    "            \n",
    "            # Fully connected layers\n",
    "            h = tf.layers.batch_normalization(h, training=self.is_training)\n",
    "            h = tf.nn.relu(h)\n",
    "            h = tf.layers.flatten(h)\n",
    "            print(h)\n",
    "            \n",
    "            h = tf.layers.dense(h, 28, kernel_initializer=tf.glorot_uniform_initializer(), \n",
    "                                activation=tf.nn.relu, name='dense1')\n",
    "            print(h)\n",
    "            h = tf.layers.dropout(h, rate=0.25, training=self.is_training, name='dropout1')\n",
    "            print(h)\n",
    "            \n",
    "            self.logits = tf.layers.dense(h, 10, kernel_initializer=tf.glorot_uniform_initializer(), \n",
    "                                          activation=tf.identity, name='dense2')\n",
    "            print(self.logits)\n",
    "            self.prediction = tf.nn.softmax(self.logits, name='softmax_prediction')\n",
    "            self.precision = tf.reduce_mean(self.prediction*self.ground_truth)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, \n",
    "                                                                                  labels=self.ground_truth))\n",
    "            self.loss += self.weight_decay()\n",
    "            \n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.train_op = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "    def weight_decay(self):\n",
    "        loss = 0\n",
    "        for v in tf.global_variables():\n",
    "            if 'Adam' in v.name:\n",
    "                continue\n",
    "            elif 'kernel' in v.name:\n",
    "                loss += self.wd_factor * tf.nn.l2_loss(v)\n",
    "        print(loss)\n",
    "        return loss\n",
    "    \n",
    "    def train_minibatch(self, samples, labels, batch_size):\n",
    "        if self.train_pointer + batch_size <= samples.shape[0]:\n",
    "            samples_minibatch = samples[self.train_pointer: self.train_pointer + batch_size]\n",
    "            labels_minibatch = labels[self.train_pointer: self.train_pointer + batch_size]\n",
    "            self.train_pointer += batch_size\n",
    "        else:\n",
    "            samples_minibatch = samples[self.train_pointer:]\n",
    "            labels_minibatch = labels[self.train_pointer: self.train_pointer + batch_size]\n",
    "            self.train_pointer = 0\n",
    "        return samples_minibatch, labels_minibatch\n",
    "\n",
    "    def train(self, train_samples, train_labels, train_batch_size, iteration_steps):\n",
    "        print('Start Training')\n",
    "        losses = []\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            for i in range(iteration_steps):\n",
    "                samples, labels = self.train_minibatch(train_samples, train_labels, train_batch_size)\n",
    "                \n",
    "                feed_dict = {self.input: samples, self.ground_truth: labels, self.is_training: True}\n",
    "                _, loss = sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "                \n",
    "                if i % 50 == 0:\n",
    "                    print(\"Minibatch loss at step {}: {}\".format(i, loss))\n",
    "                    losses.append([i, loss])\n",
    "                    \n",
    "            saver.save(sess, './model')\n",
    "        return losses\n",
    "                    \n",
    "    def test_minibatch(self, samples, labels, batch_size):\n",
    "        if self.test_pointer + batch_size <= samples.shape[0]:\n",
    "            samples_minibatch = samples[self.test_pointer: self.test_pointer + batch_size]\n",
    "            labels_minibatch = labels[self.test_pointer: self.test_pointer + batch_size]\n",
    "            self.test_pointer += batch_size\n",
    "            end_of_epoch = False\n",
    "        else:\n",
    "            samples_minibatch = samples[self.test_pointer:]\n",
    "            labels_minibatch = labels[self.test_pointer: self.test_pointer + batch_size]\n",
    "            self.test_pointer = 0\n",
    "            end_of_epoch = True\n",
    "        return samples_minibatch, labels_minibatch, end_of_epoch\n",
    "            \n",
    "    def test(self, test_samples, test_labels, test_batch_size):\n",
    "        self.test_pointer = 0\n",
    "        end_of_epoch = False\n",
    "        losses = []\n",
    "        precisions = []\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.import_meta_graph(\"./model.meta\")\n",
    "            saver.restore(sess, './model')\n",
    "            while not end_of_epoch:\n",
    "                samples, labels, end_of_epoch = self.test_minibatch(test_samples, test_labels, test_batch_size)\n",
    "                feed_dict = {self.input: samples, self.ground_truth: labels, self.is_training: False}\n",
    "                losses.append(sess.run(self.loss, feed_dict=feed_dict))\n",
    "                precisions.append(sess.run(self.precision, feed_dict=feed_dict))\n",
    "        print(\"Average test loss: {}\".format(np.mean(losses)))\n",
    "        print(\"Average test precision: {}\".format(np.mean(precisions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"layers/conv1/BiasAdd:0\", shape=(?, 7, 7, 28), dtype=float32)\n",
      "Tensor(\"layers/pool1/MaxPool:0\", shape=(?, 3, 3, 56), dtype=float32)\n",
      "Tensor(\"layers/flatten/Reshape:0\", shape=(?, 504), dtype=float32)\n",
      "Tensor(\"layers/dense1/Relu:0\", shape=(?, 28), dtype=float32)\n",
      "Tensor(\"layers/dropout1/cond/Merge:0\", shape=(?, 28), dtype=float32)\n",
      "Tensor(\"layers/dense2/Identity:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"loss/add_4:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "WD_FACTOR = 0.0\n",
    "LEARNING_RATE = 0.001\n",
    "model = MNIST_CNN(WD_FACTOR, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Minibatch loss at step 0: 2.626481056213379\n",
      "Minibatch loss at step 50: 0.4495670199394226\n",
      "Minibatch loss at step 100: 0.22456824779510498\n",
      "Minibatch loss at step 150: 0.22280184924602509\n",
      "Minibatch loss at step 200: 0.14424251019954681\n",
      "Minibatch loss at step 250: 0.1786392778158188\n",
      "Minibatch loss at step 300: 0.10426922142505646\n",
      "Minibatch loss at step 350: 0.16647326946258545\n",
      "Minibatch loss at step 400: 0.17550642788410187\n",
      "Minibatch loss at step 450: 0.14934603869915009\n",
      "Minibatch loss at step 500: 0.06552302837371826\n",
      "Minibatch loss at step 550: 0.05068168789148331\n",
      "Minibatch loss at step 600: 0.05771771818399429\n",
      "Minibatch loss at step 650: 0.07061652094125748\n",
      "Minibatch loss at step 700: 0.09996373951435089\n",
      "Minibatch loss at step 750: 0.10561545193195343\n",
      "Minibatch loss at step 800: 0.09133583307266235\n",
      "Minibatch loss at step 850: 0.03967747837305069\n",
      "Minibatch loss at step 900: 0.1409357786178589\n",
      "Minibatch loss at step 950: 0.1353156864643097\n",
      "Minibatch loss at step 1000: 0.052278440445661545\n",
      "Minibatch loss at step 1050: 0.04971937835216522\n",
      "Minibatch loss at step 1100: 0.1362321823835373\n",
      "Minibatch loss at step 1150: 0.14726319909095764\n",
      "Minibatch loss at step 1200: 0.01060505397617817\n",
      "Minibatch loss at step 1250: 0.04251568764448166\n",
      "Minibatch loss at step 1300: 0.08484410494565964\n",
      "Minibatch loss at step 1350: 0.10310755670070648\n",
      "Minibatch loss at step 1400: 0.014782343059778214\n",
      "Minibatch loss at step 1450: 0.06940300762653351\n",
      "Minibatch loss at step 1500: 0.05145518109202385\n",
      "Minibatch loss at step 1550: 0.12187056243419647\n",
      "Minibatch loss at step 1600: 0.06405770033597946\n",
      "Minibatch loss at step 1650: 0.01245881337672472\n",
      "Minibatch loss at step 1700: 0.014947250485420227\n",
      "Minibatch loss at step 1750: 0.07459593564271927\n",
      "Minibatch loss at step 1800: 0.053169332444667816\n",
      "Minibatch loss at step 1850: 0.05137448012828827\n",
      "Minibatch loss at step 1900: 0.01894041709601879\n",
      "Minibatch loss at step 1950: 0.04541940614581108\n",
      "Minibatch loss at step 2000: 0.047848500311374664\n",
      "Minibatch loss at step 2050: 0.02827657200396061\n",
      "Minibatch loss at step 2100: 0.07604952901601791\n",
      "Minibatch loss at step 2150: 0.004915050696581602\n",
      "Minibatch loss at step 2200: 0.08172813057899475\n",
      "Minibatch loss at step 2250: 0.0743606761097908\n",
      "Minibatch loss at step 2300: 0.039611928164958954\n",
      "Minibatch loss at step 2350: 0.024078557267785072\n",
      "Minibatch loss at step 2400: 0.04232292249798775\n",
      "Minibatch loss at step 2450: 0.05629342049360275\n",
      "Minibatch loss at step 2500: 0.010986979119479656\n",
      "Minibatch loss at step 2550: 0.025253554806113243\n",
      "Minibatch loss at step 2600: 0.03263300284743309\n",
      "Minibatch loss at step 2650: 0.006111824419349432\n",
      "Minibatch loss at step 2700: 0.03179174289107323\n",
      "Minibatch loss at step 2750: 0.03830723837018013\n",
      "Minibatch loss at step 2800: 0.011120947077870369\n",
      "Minibatch loss at step 2850: 0.023714879527688026\n",
      "Minibatch loss at step 2900: 0.022013772279024124\n",
      "Minibatch loss at step 2950: 0.008689740672707558\n",
      "Minibatch loss at step 3000: 0.02216464653611183\n",
      "Minibatch loss at step 3050: 0.006728807929903269\n",
      "Minibatch loss at step 3100: 0.01254347525537014\n",
      "Minibatch loss at step 3150: 0.06255599111318588\n",
      "Minibatch loss at step 3200: 0.010448263958096504\n",
      "Minibatch loss at step 3250: 0.053478144109249115\n",
      "Minibatch loss at step 3300: 0.008768679574131966\n",
      "Minibatch loss at step 3350: 0.010715613141655922\n",
      "Minibatch loss at step 3400: 0.02322237566113472\n",
      "Minibatch loss at step 3450: 0.08618993312120438\n",
      "Minibatch loss at step 3500: 0.05810261517763138\n",
      "Minibatch loss at step 3550: 0.011856968514621258\n",
      "Minibatch loss at step 3600: 0.015711504966020584\n",
      "Minibatch loss at step 3650: 0.029555872082710266\n",
      "Minibatch loss at step 3700: 0.030437074601650238\n",
      "Minibatch loss at step 3750: 0.0015048510394990444\n",
      "Minibatch loss at step 3800: 0.012111787684261799\n",
      "Minibatch loss at step 3850: 0.01610272005200386\n",
      "Minibatch loss at step 3900: 0.0015738389920443296\n",
      "Minibatch loss at step 3950: 0.007101257797330618\n",
      "Minibatch loss at step 4000: 0.05908755213022232\n",
      "Minibatch loss at step 4050: 0.004590339493006468\n",
      "Minibatch loss at step 4100: 0.0030226637609302998\n",
      "Minibatch loss at step 4150: 0.005377925932407379\n",
      "Minibatch loss at step 4200: 0.025212109088897705\n",
      "Minibatch loss at step 4250: 0.012750877067446709\n",
      "Minibatch loss at step 4300: 0.04390763118863106\n",
      "Minibatch loss at step 4350: 0.012622683309018612\n",
      "Minibatch loss at step 4400: 0.0021019689738750458\n",
      "Minibatch loss at step 4450: 0.03475392609834671\n",
      "Minibatch loss at step 4500: 0.01129370927810669\n",
      "Minibatch loss at step 4550: 0.007612114772200584\n",
      "Minibatch loss at step 4600: 0.006154051050543785\n",
      "Minibatch loss at step 4650: 0.0044541251845657825\n",
      "Minibatch loss at step 4700: 0.11564209312200546\n",
      "Minibatch loss at step 4750: 0.010201490484178066\n",
      "Minibatch loss at step 4800: 0.01714714802801609\n",
      "Minibatch loss at step 4850: 0.046772416681051254\n",
      "Minibatch loss at step 4900: 0.05485806241631508\n",
      "Minibatch loss at step 4950: 0.03133990243077278\n",
      "Minibatch loss at step 5000: 0.03146188706159592\n",
      "Minibatch loss at step 5050: 0.009590605273842812\n",
      "Minibatch loss at step 5100: 0.003974642604589462\n",
      "Minibatch loss at step 5150: 0.0012860613642260432\n",
      "Minibatch loss at step 5200: 0.004066807217895985\n",
      "Minibatch loss at step 5250: 0.02954796329140663\n",
      "Minibatch loss at step 5300: 0.008377310819923878\n",
      "Minibatch loss at step 5350: 0.0076365661807358265\n",
      "Minibatch loss at step 5400: 0.006232011131942272\n",
      "Minibatch loss at step 5450: 0.016073638573288918\n",
      "Minibatch loss at step 5500: 0.018599502742290497\n",
      "Minibatch loss at step 5550: 0.009121273644268513\n",
      "Minibatch loss at step 5600: 0.02788470685482025\n",
      "Minibatch loss at step 5650: 0.00660347193479538\n",
      "Minibatch loss at step 5700: 0.03380715474486351\n",
      "Minibatch loss at step 5750: 0.03558480739593506\n",
      "Minibatch loss at step 5800: 0.037723056972026825\n",
      "Minibatch loss at step 5850: 0.004316059872508049\n",
      "Minibatch loss at step 5900: 0.034629639238119125\n",
      "Minibatch loss at step 5950: 0.03447933495044708\n",
      "Minibatch loss at step 6000: 0.00575537234544754\n",
      "Minibatch loss at step 6050: 0.019042929634451866\n",
      "Minibatch loss at step 6100: 0.015000846236944199\n",
      "Minibatch loss at step 6150: 0.02168978936970234\n",
      "Minibatch loss at step 6200: 0.0027978746220469475\n",
      "Minibatch loss at step 6250: 0.0017225712072104216\n",
      "Minibatch loss at step 6300: 0.002650079783052206\n",
      "Minibatch loss at step 6350: 0.020478805527091026\n",
      "Minibatch loss at step 6400: 0.0018898241687566042\n",
      "Minibatch loss at step 6450: 0.006180453114211559\n",
      "Minibatch loss at step 6500: 0.009533356875181198\n",
      "Minibatch loss at step 6550: 0.029824387282133102\n",
      "Minibatch loss at step 6600: 0.04148087650537491\n",
      "Minibatch loss at step 6650: 0.023029372096061707\n",
      "Minibatch loss at step 6700: 0.0018005932215601206\n",
      "Minibatch loss at step 6750: 0.02740759775042534\n",
      "Minibatch loss at step 6800: 0.053347110748291016\n",
      "Minibatch loss at step 6850: 0.008447708562016487\n",
      "Minibatch loss at step 6900: 0.050640132278203964\n",
      "Minibatch loss at step 6950: 0.0008394003962166607\n",
      "Minibatch loss at step 7000: 0.0012571559054777026\n",
      "Minibatch loss at step 7050: 0.004897136706858873\n",
      "Minibatch loss at step 7100: 0.002575809368863702\n",
      "Minibatch loss at step 7150: 0.01786840334534645\n",
      "Minibatch loss at step 7200: 0.01991097442805767\n",
      "Minibatch loss at step 7250: 0.018390130251646042\n",
      "Minibatch loss at step 7300: 0.004113676492124796\n",
      "Minibatch loss at step 7350: 0.00048594054533168674\n",
      "Minibatch loss at step 7400: 0.0015067404601722956\n",
      "Minibatch loss at step 7450: 0.01170708704739809\n",
      "Minibatch loss at step 7500: 0.00017628876958042383\n",
      "Minibatch loss at step 7550: 0.012320060282945633\n",
      "Minibatch loss at step 7600: 0.004755962640047073\n",
      "Minibatch loss at step 7650: 0.004551602527499199\n",
      "Minibatch loss at step 7700: 0.0012649440905079246\n",
      "Minibatch loss at step 7750: 0.03026561439037323\n",
      "Minibatch loss at step 7800: 0.013828648254275322\n",
      "Minibatch loss at step 7850: 0.02330823987722397\n",
      "Minibatch loss at step 7900: 0.016604797914624214\n",
      "Minibatch loss at step 7950: 0.020389152690768242\n",
      "Minibatch loss at step 8000: 0.008829979225993156\n",
      "Minibatch loss at step 8050: 0.004333220887929201\n",
      "Minibatch loss at step 8100: 0.0006217895424924791\n",
      "Minibatch loss at step 8150: 0.01999545469880104\n",
      "Minibatch loss at step 8200: 0.00588248809799552\n",
      "Minibatch loss at step 8250: 0.0073867784813046455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 8300: 0.00829398538917303\n",
      "Minibatch loss at step 8350: 0.011044622398912907\n",
      "Minibatch loss at step 8400: 0.0018712211167439818\n",
      "Minibatch loss at step 8450: 0.022888660430908203\n",
      "Minibatch loss at step 8500: 0.007007864769548178\n",
      "Minibatch loss at step 8550: 0.012913266196846962\n",
      "Minibatch loss at step 8600: 0.04425138980150223\n",
      "Minibatch loss at step 8650: 0.017760509625077248\n",
      "Minibatch loss at step 8700: 0.0005477896775119007\n",
      "Minibatch loss at step 8750: 0.008988309651613235\n",
      "Minibatch loss at step 8800: 0.03547430783510208\n",
      "Minibatch loss at step 8850: 0.011317454278469086\n",
      "Minibatch loss at step 8900: 0.0024641507770866156\n",
      "Minibatch loss at step 8950: 0.00928648840636015\n",
      "Minibatch loss at step 9000: 0.0006924492190591991\n",
      "Minibatch loss at step 9050: 0.0077907619997859\n",
      "Minibatch loss at step 9100: 0.0017951929476112127\n",
      "Minibatch loss at step 9150: 0.007771703880280256\n",
      "Minibatch loss at step 9200: 0.004882729146629572\n",
      "Minibatch loss at step 9250: 0.003383169649168849\n",
      "Minibatch loss at step 9300: 0.009249303489923477\n",
      "Minibatch loss at step 9350: 0.002184131182730198\n",
      "Minibatch loss at step 9400: 0.017613142728805542\n",
      "Minibatch loss at step 9450: 0.00041380157927051187\n",
      "Minibatch loss at step 9500: 0.01182333193719387\n",
      "Minibatch loss at step 9550: 0.0010199955431744456\n",
      "Minibatch loss at step 9600: 0.0004248185141477734\n",
      "Minibatch loss at step 9650: 0.015339020639657974\n",
      "Minibatch loss at step 9700: 0.0010307877091690898\n",
      "Minibatch loss at step 9750: 0.004873951897025108\n",
      "Minibatch loss at step 9800: 0.007083384320139885\n",
      "Minibatch loss at step 9850: 0.023817917332053185\n",
      "Minibatch loss at step 9900: 0.003985177725553513\n",
      "Minibatch loss at step 9950: 0.08780702203512192\n",
      "Training time: 49.69185447692871s\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 128\n",
    "ITERATIONS = 10000\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "losses = model.train(train_set, train_labels_onehot, TRAIN_BATCH_SIZE, ITERATIONS)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Training time: {}s\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "losses=np.array(losses)\n",
    "iterations = losses[:, 0]\n",
    "train_loss = losses[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, train_loss, 'b-')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Average test loss: 0.007404327392578125\n",
      "Average test loss: 0.09966221451759338\n"
     ]
    }
   ],
   "source": [
    "TEST_BATCH_SIZE = 128\n",
    "\n",
    "model.test(train_set, train_labels_onehot, TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Average test loss: 0.06277027726173401\n",
      "Average test loss: 0.09861195832490921\n"
     ]
    }
   ],
   "source": [
    "TEST_BATCH_SIZE = 128\n",
    "\n",
    "model.test(test_set, test_labels_onehot, TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
